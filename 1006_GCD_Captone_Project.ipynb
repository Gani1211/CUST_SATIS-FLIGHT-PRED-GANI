{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gani1211/CUST_SATIS-FLIGHT-PRED-GANI/blob/main/1006_GCD_Captone_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38a3782c",
      "metadata": {
        "id": "38a3782c"
      },
      "source": [
        "<center><img src=\"https://projects.insaid.co/capstone2/companylogo.png\" width=\"240\" height=\"100\" /></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7169dc0f",
      "metadata": {
        "id": "7169dc0f"
      },
      "source": [
        "---\n",
        "# **Table of Contents**\n",
        "---\n",
        "\n",
        "**1.** [**Introduction**](#Section1)<br>\n",
        "**2.** [**Problem Statement**](#Section2)<br>\n",
        "**3.** [**Installing & Importing Libraries**](#Section3)<br>\n",
        "  - **3.1** [**Installing Libraries**](#Section31)\n",
        "  - **3.2** [**Upgrading Libraries**](#Section32)\n",
        "  - **3.3** [**Importing Libraries**](#Section33)\n",
        "\n",
        "**4.** [**Data Acquisition & Description**](#Section4)<br>\n",
        "  - **4.1** [**Data Description**](#Section41)\n",
        "  --**4.1.1** [**Connecting mySQL**](#Section411)\n",
        "  - **4.2** [**Data Information**](#Section42)\n",
        "\n",
        "**5.** [**Data Pre-processing**](#Section5)<br>\n",
        "  - **5.1** [**Data Pre-profiling**](#Section51)<br>\n",
        "  - **5.2** [**Data Cleaning**](#Section52)<br>\n",
        "  - **5.3** [**Data Post-profiling**](#Section53)<br>\n",
        "\n",
        "**6.** [**Exploratory Data Analysis**](#Section6)<br>\n",
        "**7.** [**Data Post-Processing**](#Section7)<br>\n",
        "  - **7.1** [**Data Encoding**](#Section71)<br>\n",
        "  - **7.2** [**Data Scaling**](#Section72)<br>\n",
        "  - **7.3** [**Data Preparation**](#Section73)<br>\n",
        "\n",
        "**8.** [**Model Development & Evaluation**](#Section8)<br>\n",
        "**9.** [**Unseen Data**](#Section9)<br>\n",
        "  - **9.1** [**Processing**](#Section91)<br>\n",
        "  - **9.2** [**Applying selected Model**](#Section92)<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2484c8a1",
      "metadata": {
        "id": "2484c8a1"
      },
      "source": [
        "---\n",
        "<a name = Section1></a>\n",
        "# **1. Introduction**\n",
        "---\n",
        "\n",
        "**<h2>HRAdvisories:</h2>**\n",
        "**<h3>Company Introduction:</h3>**\n",
        "- Client for this project is the HR Department at a software company.\n",
        "  - They want to try a new initiative to retain employees, the idea is to use data to predict whether an employee is likely to leave.\n",
        "  - Once these employees are identified, HR can be more proactive in reaching out to them before it's too late.\n",
        "- They only want to deal with the data that is related to permanent employees.\n",
        "\n",
        "__________________________________________________\n",
        "**<h3>Current Practice:</h3>**\n",
        "Once an employee leaves, he or she is taken an interview with the name “exit interview” and shares reasons for leaving. The HR Department then tries and learns insights from the interview and makes changes accordingly.\n",
        "\n",
        "**<h3>This suffers from the following problems:</h3>**\n",
        "- This approach is that it's too haphazard. The quality of insight gained from an interview depends heavily on the skill of the interviewer.\n",
        "- The second problem is these insights can't be aggregated and interlaced across all employees who have left.\n",
        "- The third is that it is too late by the time the proposed policy changes take effect.\n",
        "- The HR department has hired you as data science consultants. They want to supplement their exit interviews with a more proactive approach.\n",
        "\n",
        "<center><img src=\"https://projects.insaid.co/capstone2/hr.png\"></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "997ff614",
      "metadata": {
        "id": "997ff614"
      },
      "source": [
        "---\n",
        "<a name = Section2></a>\n",
        "# **2. Problem Statement**\n",
        "---\n",
        "\n",
        "- we have datasets of past employees and their status (still employed or already left).\n",
        "- Task is to build a classification model using the datasets.\n",
        "- Because there was no machine learning model for this problem in the company, we don’t have quantifiable win condition. We need to build the best possible model.\n",
        "\n",
        "\n",
        "<center><img src=\"https://www.aihr.com/wp-content/uploads/https-www.analyticsinhr.com-blog-what-drives-employee-turnover-01.png\"></center>\n",
        "\n",
        "\n",
        "- So, the primary objectives are to:\n",
        "     - Perform an Exploratory Data Analysis of the dataset to understand the correlations of customer churning\n",
        "     - Build the Classification modle to predict whether a customer will churn or not.\n",
        "\n",
        "- Project Deliverables\n",
        "     - Deliverable: Predict whether an employee will stay or leave.\n",
        "     - Machine learning task: Classification\n",
        "     - Target variable: Status (Employed/Left)\n",
        "     - Win condition: N/A (best possible model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5e587ab",
      "metadata": {
        "id": "a5e587ab"
      },
      "source": [
        "---\n",
        "<a name = Section3></a>\n",
        "# **3. Installing & Importing Libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80eca917",
      "metadata": {
        "id": "80eca917"
      },
      "source": [
        "<a name = Section31></a>\n",
        "### **3.1 Installing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "647e7633",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "647e7633",
        "outputId": "03fdeb73-b0ea-423b-cc5a-45544f05de95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sweetviz\n",
            "  Downloading sweetviz-2.3.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (3.7.1)\n",
            "Requirement already satisfied: tqdm>=4.43.0 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (4.66.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (1.13.1)\n",
            "Requirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (3.1.4)\n",
            "Requirement already satisfied: importlib-resources>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (6.4.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.1->sweetviz) (3.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->sweetviz) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->sweetviz) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.3->sweetviz) (1.16.0)\n",
            "Downloading sweetviz-2.3.1-py3-none-any.whl (15.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sweetviz\n",
            "Successfully installed sweetviz-2.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sweetviz                                             # Package for high-density visualizations to kickstart EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b77b91c4",
      "metadata": {
        "id": "b77b91c4"
      },
      "source": [
        "<a name = Section32></a>\n",
        "### **3.2 Upgrading Libraries**\n",
        "\n",
        "- **After upgrading** the libraries, you need to **restart the runtime** to make the libraries in sync.\n",
        "\n",
        "- Make sure **not to execute** the cell above (3.1) and below (3.2) again after restarting the runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8422b2c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8422b2c9",
        "outputId": "74ac5e00-231f-4c83-c5dd-e595a5cce0b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mysql.connector\n",
            "  Downloading mysql-connector-2.2.9.tar.gz (11.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mysql.connector\n",
            "  Building wheel for mysql.connector (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysql.connector: filename=mysql_connector-2.2.9-cp310-cp310-linux_x86_64.whl size=247948 sha256=68795694e19bda1c4659c35cdc20b29f140faa0e3d56cf0ed6f6eeace97cfe99\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/48/9b/da67ff1a18fe8e9d428f9b1a177716d4a7d363d2bbe83bf6cf\n",
            "Successfully built mysql.connector\n",
            "Installing collected packages: mysql.connector\n",
            "Successfully installed mysql.connector-2.2.9\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install mysql.connector"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8488acc0",
      "metadata": {
        "id": "8488acc0"
      },
      "source": [
        "<a name = Section33></a>\n",
        "### **3.3 Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec2761af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ec2761af",
        "outputId": "d7946945-9e4e-4f08-d339-5990611c0874"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import mysql.connector\n",
        "import os\n",
        "#------------------------------------------------------------------------------------------------------------------------------\n",
        "import pandas as pd                                                 # Importing for panel data analysis\n",
        "                         # Import Pandas Profiling (To generate Univariate Analysis)\n",
        "pd.set_option('display.max_columns', None)                          # Unfolding hidden features if the cardinality is high\n",
        "pd.set_option('display.max_rows', None)                             # Unfolding hidden data points if the cardinality is high\n",
        "pd.set_option('mode.chained_assignment', None)                      # Removing restriction over chained assignments operations\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "import numpy as np                                                  # Importing package numpys (For Numerical Python)\n",
        "from scipy.stats import randint as sp_randint                       # For initializing random integer values\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "import matplotlib.pyplot as plt                                     # Importing pyplot interface using matplotlib\n",
        "import seaborn as sns                                               # Importin seaborm library for interactive visualization\n",
        "%matplotlib inline\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "from sklearn.preprocessing import StandardScaler                    # To scaled data with mean 0 and variance 1\n",
        "from sklearn.model_selection import train_test_split                # To split the data in training and testing part\n",
        "from sklearn.model_selection import RandomizedSearchCV              # To find best hyperparamter setting for the algorithm\n",
        "from sklearn.ensemble import RandomForestClassifier                 # To implement random forest classifier\n",
        "from sklearn.tree import DecisionTreeClassifier                     # To implement decision tree classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier                  # To instantiate a KNN Classifier\n",
        "from sklearn.linear_model import LogisticRegression                 # To instantiate a Logistic Regression Classifier\n",
        "from sklearn.naive_bayes import GaussianNB                          # To instantiate a Naive Bayes Classifier\n",
        "from sklearn.metrics import accuracy_score                          # To calculate the accuracy of classifiers\n",
        "from sklearn.model_selection import KFold                           # To create k folds for cross validation\n",
        "from sklearn.model_selection import cross_validate                  # To calculate cross validation scores\n",
        "from sklearn.model_selection import GridSearchCV                    # To tune the models\n",
        "from sklearn.metrics import classification_report                   # To generate classification report\n",
        "#from sklearn.metrics import plot_confusion_matrix                   # To plot confusion matrix\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "#import pydotplus                                                    # To generate pydot file\n",
        "from IPython.display import Image                                   # To generate image using pydot file\n",
        "import sweetviz as svz\n",
        "\n",
        "from plotly.subplots import make_subplots                           # Importing to create subplots in plotly\n",
        "import plotly.graph_objs as go\n",
        "import plotly.offline as py\n",
        "py.init_notebook_mode(connected=True)\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "import time                                                         # Importing to calculate time\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "import warnings                                                     # Importing warning to disable runtime warnings\n",
        "warnings.filterwarnings(\"ignore\")                                   # Warnings will appear only once"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70cf5179",
      "metadata": {
        "id": "70cf5179"
      },
      "source": [
        "---\n",
        "<a name = Section4></a>\n",
        "# **4. Data Acquisition & Description**\n",
        "---\n",
        "\n",
        "\n",
        "- The first dataset consists of the information about department_data.\n",
        "- This dataset contains information about each department. The schema of the dataset is as follows:\n",
        "\n",
        "\n",
        "| ID | Feature Name | Description of the feature |\n",
        "| :-- | :--| :--|\n",
        "|01| **dept_id**   | Unique Department Code|\n",
        "|02| **dept_name**      | Name of the Department|\n",
        "|03| **dept_head**        |Name of the Head of the Department|\n",
        "\n",
        "| Records | Features | Dataset Size |\n",
        "| :-- | :-- | :-- |\n",
        "| 3252950 | 7 | 286 MB |\n",
        "\n",
        "- The Second dataset consists of the information about employee_details_data.\n",
        "- This dataset consists of Employee ID, their Age, Gender and Marital Status. The schema of this dataset is as follows:\n",
        "\n",
        "| Records | Features | Dataset Size |\n",
        "| :-- | :-- | :-- |\n",
        "| 87726 | 3 | 2 MB |\n",
        "\n",
        "| ID | Feature Name | Description of the feature |\n",
        "| :-- | :--| :--|\n",
        "|01| **employee_id**      | Unique ID Number for each employee|\n",
        "|02| **age**      | Age of the employee|\n",
        "|03| **gender**        |Gender of the employee|\n",
        "|04| **marital_status**        |Marital Status of the employee|\n",
        "\n",
        "- The third dataset consists of the information about employee_data.\n",
        "- This dataset consists of each employee’s Administrative Information, Workload Information, Mutual Evaluation Information and Status\n",
        "\n",
        "\n",
        "| Records | Features | Dataset Size |\n",
        "| :-- | :-- | :-- |\n",
        "| 74645 | 4 | 2.3 MB |\n",
        "\n",
        "| ID | Feature Name | Description of the feature |\n",
        "| :-- | :--| :--|\n",
        "|01| **status**      | Current employment status (Employed / Left)|\n",
        "|02| **department**      | Department to which the employees belong(ed) to|\n",
        "|03| **salary**        | Salary level with respect to rest of their department|\n",
        "|04| **tenure**        | Number of years at the company|\n",
        "|05| **recently_promoted**      | Was the employee promoted in the last 3 years?|\n",
        "|06| **employee_id**      | Unique ID Number for each employee|\n",
        "|07| **n_projects**        | Number of projects employee has worked on|\n",
        "|08| **avg_monthly_hrs**        | Average number of hours worked per month|\n",
        "|09| **satisfaction**      |Score for employee’s satisfaction with the company (higher is better)|\n",
        "|10| **last_evaluation**      |Score for most recent evaluation of employee (higher is better)|\n",
        "|11| **filed_complaint**        |Has the employee filed a formal complaint in the last 3 years?|\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name = Section411></a>\n",
        "### **4.1.1 Connecting mySQL**"
      ],
      "metadata": {
        "id": "Ik9xh5SoNhbJ"
      },
      "id": "Ik9xh5SoNhbJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afe3c936",
      "metadata": {
        "id": "afe3c936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "5470c48b-8186-446d-8ac2-457f36a0a463"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "InterfaceError",
          "evalue": "2003: Can't connect to MySQL server on 'cpanel.insaid.co:3306' (110 Connection timed out)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mysql/connector/network.py\u001b[0m in \u001b[0;36mopen_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msockaddr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTimeoutError\u001b[0m: [Errno 110] Connection timed out",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInterfaceError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-1dbfa0190033>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Connect to server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m cnx = mysql.connector.connect(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpanel.insaid.co'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'student'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'student'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mysql/connector/__init__.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mCMySQLConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mMySQLConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0mConnect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnect\u001b[0m  \u001b[0;31m# pylint: disable=C0103\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mysql/connector/connection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mysql/connector/abstracts.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mysql/connector/connection.py\u001b[0m in \u001b[0;36m_open_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMySQLProtocol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         self._do_auth(self._user, self._password,\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mysql/connector/network.py\u001b[0m in \u001b[0;36mopen_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msockaddr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             raise errors.InterfaceError(\n\u001b[0m\u001b[1;32m    512\u001b[0m                 errno=2003, values=(self.get_address(), _strioerror(err)))\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInterfaceError\u001b[0m: 2003: Can't connect to MySQL server on 'cpanel.insaid.co:3306' (110 Connection timed out)"
          ]
        }
      ],
      "source": [
        "# Connect to server\n",
        "cnx = mysql.connector.connect(\n",
        "    host='cpanel.insaid.co',\n",
        "    user='student',\n",
        "    password='student',\n",
        "    database = 'Capstone2')\n",
        "\n",
        "# Get a cursor\n",
        "cur = cnx.cursor()\n",
        "\n",
        "query1 = (\"SELECT * FROM department_data\")\n",
        "query2 = (\"SELECT * FROM employee_details_data\")\n",
        "query3 = (\"SELECT * FROM employee_data\")\n",
        "\n",
        "\"\"\"\n",
        "cur.execute(query)\n",
        "\n",
        "myresult = cur.fetchall()\n",
        "\n",
        "for x in myresult:\n",
        "  print(x)\n",
        "\"\"\"\n",
        "\n",
        "df_dep = pd.read_sql(query1,cnx)\n",
        "df_emp1 = pd.read_sql(query2,cnx)\n",
        "df_emp2 = pd.read_sql(query3,cnx)\n",
        "#df = pd.read_sql('SELECT * FROM gender_age_train', con=cnx)\n",
        "\n",
        "cur.close()\n",
        "cnx.close()\n",
        "# Execute a query"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name = Section42></a>\n",
        "### **4.2 Data Info**"
      ],
      "metadata": {
        "id": "oIrV4P9rNv8R"
      },
      "id": "oIrV4P9rNv8R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f82f810",
      "metadata": {
        "scrolled": true,
        "id": "4f82f810"
      },
      "outputs": [],
      "source": [
        "# Get the dimesions of data\n",
        "print('Shape of the dataset:', df_dep.shape)\n",
        "# Output all the 11 data rows\n",
        "df_dep.head(11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b63b648",
      "metadata": {
        "id": "5b63b648"
      },
      "outputs": [],
      "source": [
        "#renaming the dept_id to department for merging with employee data further down the process\n",
        "df_dep.rename(columns = {'dept_id':'department'}, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "893c3488",
      "metadata": {
        "id": "893c3488"
      },
      "outputs": [],
      "source": [
        "df_dep.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "894f59e9",
      "metadata": {
        "id": "894f59e9"
      },
      "outputs": [],
      "source": [
        "# Get the dimesions of employee details\n",
        "print('Shape of the dataset:', df_emp1.shape)\n",
        "df_emp1.info()\n",
        "# Output first 5 data rows\n",
        "df_emp1.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking Duplicates in emp1\n",
        "duplicate1 = df_emp1[df_emp1.duplicated('employee_id')]\n",
        "duplicate1"
      ],
      "metadata": {
        "id": "UKGrGQr0jK0R"
      },
      "id": "UKGrGQr0jK0R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bfb2628",
      "metadata": {
        "scrolled": true,
        "id": "2bfb2628"
      },
      "outputs": [],
      "source": [
        "# Get the dimesions of employee data\n",
        "print('Shape of the dataset:', df_emp2.shape)\n",
        "df_emp2.info()\n",
        "# Output first 5 data rows\n",
        "df_emp2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a49c6d60",
      "metadata": {
        "scrolled": false,
        "id": "a49c6d60"
      },
      "outputs": [],
      "source": [
        "print (df_emp2['department'].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8836f72b",
      "metadata": {
        "id": "8836f72b"
      },
      "source": [
        "### Replacing dept_name '-IT'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57f59926",
      "metadata": {
        "id": "57f59926"
      },
      "outputs": [],
      "source": [
        "df_emp2=df_emp2.replace(['-IT'],['D00-IT'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a6f2fc2",
      "metadata": {
        "id": "8a6f2fc2"
      },
      "outputs": [],
      "source": [
        "print (df_emp2['department'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing Duplicate entries by selected parameters"
      ],
      "metadata": {
        "id": "j1YaknjsnWiO"
      },
      "id": "j1YaknjsnWiO"
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking Duplicates in emp2\n",
        "duplicate = df_emp2[df_emp2.duplicated(['employee_id', 'department', 'last_evaluation', 'satisfaction'])]\n",
        "duplicate"
      ],
      "metadata": {
        "id": "w47itNPy8lQh"
      },
      "id": "w47itNPy8lQh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Observations:**\n",
        "There are 29 employee_id records which are duplicated based on the unique key selection"
      ],
      "metadata": {
        "id": "U8NaTWg5dwAw"
      },
      "id": "U8NaTWg5dwAw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dropping duplicates"
      ],
      "metadata": {
        "id": "csOu-FZ1fML0"
      },
      "id": "csOu-FZ1fML0"
    },
    {
      "cell_type": "code",
      "source": [
        "df_emp2.drop_duplicates(subset =['employee_id', 'department', 'last_evaluation', 'satisfaction'],\n",
        "                     keep = 'first', inplace = True)"
      ],
      "metadata": {
        "id": "vNj0cuq5jfmP"
      },
      "id": "vNj0cuq5jfmP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_emp2.info()"
      ],
      "metadata": {
        "id": "eyvMRvKxkefk"
      },
      "id": "eyvMRvKxkefk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ef74a551",
      "metadata": {
        "id": "ef74a551"
      },
      "source": [
        "### merging using left join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f24036af",
      "metadata": {
        "id": "f24036af"
      },
      "outputs": [],
      "source": [
        "merge1=pd.merge(df_emp2, df_emp1, on=\"employee_id\", how='left')\n",
        "print('Shape of the merge1 dataset:', merge1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f62f495d",
      "metadata": {
        "id": "f62f495d"
      },
      "outputs": [],
      "source": [
        "merge1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3d66865",
      "metadata": {
        "id": "c3d66865"
      },
      "outputs": [],
      "source": [
        "pre_final=pd.merge(merge1, df_dep, on=\"department\", how='left')\n",
        "print('Shape of the final merged dataset:', pre_final.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final=pre_final"
      ],
      "metadata": {
        "id": "daeRNzuSVGXh"
      },
      "id": "daeRNzuSVGXh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65e6f783",
      "metadata": {
        "scrolled": true,
        "id": "65e6f783"
      },
      "outputs": [],
      "source": [
        "final.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9385d4c5",
      "metadata": {
        "scrolled": true,
        "id": "9385d4c5"
      },
      "outputs": [],
      "source": [
        "#sns.pairplot(final, hue=\"status\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1c3062b",
      "metadata": {
        "id": "d1c3062b"
      },
      "source": [
        "<a name = Section5></a>\n",
        "\n",
        "---\n",
        "# **5. Data Pre-Processing**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a0d93ce",
      "metadata": {
        "id": "6a0d93ce"
      },
      "source": [
        "<a name = Section51></a>\n",
        "### **5.1 Data Pre-Profiling**\n",
        "\n",
        "- For **quick analysis** Sweetviz Auto EDA is very handy.\n",
        "\n",
        "- Generates profile reports from a pandas DataFrame.\n",
        "\n",
        "- For each column **statistics** are presented in an interactive HTML report."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Need to be run only in JupyterNotebook"
      ],
      "metadata": {
        "id": "1uGj0cYyOxF7"
      },
      "id": "1uGj0cYyOxF7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8ad71cb",
      "metadata": {
        "scrolled": true,
        "id": "a8ad71cb"
      },
      "outputs": [],
      "source": [
        "import sweetviz as svz\n",
        "sweet_report = svz.analyze(final)\n",
        "sweet_report.show_html('employee_attrition.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b882ad6",
      "metadata": {
        "scrolled": true,
        "id": "9b882ad6"
      },
      "outputs": [],
      "source": [
        "final.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b5871f1",
      "metadata": {
        "id": "5b5871f1"
      },
      "outputs": [],
      "source": [
        "#Data Overview\n",
        "print(\"Rows:\",final.shape[0])\n",
        "print (\"Columns:\",final.shape[1])\n",
        "print (\"\\nFeatures:\\n\",final.columns.tolist())\n",
        "print (\"\\nMissingvalues:\",final.isnull().sum().values.sum())\n",
        "print (\"\\nUnique values:\\n\",final.nunique())\n",
        "print (final['status'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<a name = Section52></a>\n",
        "# **5.2. Data Cleaning**\n",
        "---"
      ],
      "metadata": {
        "id": "qTJkLn8Fetw1"
      },
      "id": "qTJkLn8Fetw1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Missing values:**\n",
        " filed_complaint(86%) and recently_promoted (98%) have large missing values. last_evaluation (11%), dept_name, dept_head, department (5%) and satisfaction, tenure has 1% missing values.\n"
      ],
      "metadata": {
        "id": "bWreI6Zzetrd"
      },
      "id": "bWreI6Zzetrd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of tenure missing"
      ],
      "metadata": {
        "id": "S1KQRprAToLQ"
      },
      "id": "S1KQRprAToLQ"
    },
    {
      "cell_type": "code",
      "source": [
        "final_tenure = final[final['tenure'].isnull()]\n",
        "\n",
        "tenure = final_tenure.groupby(['gender', 'department']).size().unstack(fill_value=0)\n",
        "\n",
        "final_tenure.info()"
      ],
      "metadata": {
        "id": "Gicoh8HfTKGI"
      },
      "id": "Gicoh8HfTKGI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tenure"
      ],
      "metadata": {
        "id": "GjDYs7WaTfDQ"
      },
      "id": "GjDYs7WaTfDQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###evaluation of Satisfaction score"
      ],
      "metadata": {
        "id": "rZuGQUqoH9d4"
      },
      "id": "rZuGQUqoH9d4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Before we impute missing satisfaction score, it is important know which departments are having missing satisfaction score, so that we can get the mean scores of that department to impute missing values"
      ],
      "metadata": {
        "id": "YXIXpjhIH1cu"
      },
      "id": "YXIXpjhIH1cu"
    },
    {
      "cell_type": "code",
      "source": [
        "final_sat = final[final['satisfaction'].isnull()]\n",
        "\n",
        "fat = final_sat.groupby([#'filed_complaint',\n",
        "    'status', 'department']).size().unstack(fill_value=0)\n",
        "\n",
        "fat"
      ],
      "metadata": {
        "id": "jjF5wbs8H6KJ"
      },
      "id": "jjF5wbs8H6KJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Handling of filed-complaint, recently_promoted, last_evaluation, satisfaction and tenure\n",
        "** Based on the Observation of each individual items listed above, most of this features are missing for temporary employees and \"filed_complaint\" and  \"recently_promoted\" can be filled with 0 and Mean values for the columns (last_evaluation, satisfaction and tenure."
      ],
      "metadata": {
        "id": "KFIEwdxtiFCg"
      },
      "id": "KFIEwdxtiFCg"
    },
    {
      "cell_type": "code",
      "source": [
        "values = {\"filed_complaint\" : 0, \"recently_promoted\" : 0}\n",
        "final.fillna(value = values, inplace = True)\n",
        "final.info()"
      ],
      "metadata": {
        "id": "RJHU28seiIqM"
      },
      "id": "RJHU28seiIqM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Replacing with mean\n",
        "final['last_evaluation'].fillna(value=final['last_evaluation'].mean(), inplace=True)\n",
        "final['satisfaction'].fillna(value=final['satisfaction'].mean(), inplace=True)\n",
        "final['tenure'].fillna(value=final['tenure'].mean(), inplace=True)\n",
        "final.info()"
      ],
      "metadata": {
        "id": "5mwdRWWNx50e"
      },
      "id": "5mwdRWWNx50e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filling the missing values of department feature with the mode of the feature."
      ],
      "metadata": {
        "id": "DdSq2FKvp1VT"
      },
      "id": "DdSq2FKvp1VT"
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling the missing values of department feature with the mode of the feature.\n",
        "final['department'] = final['department'].fillna(value=final['department'].mode()[0])\n",
        "final['dept_name'] = final['dept_name'].fillna(value=final['dept_name'].mode()[0])\n",
        "final['dept_head'] = final['dept_head'].fillna(value=final['dept_head'].mode()[0])\n",
        "final.info()"
      ],
      "metadata": {
        "id": "_p-NeJzrjYwP"
      },
      "id": "_p-NeJzrjYwP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling of missing values for gender, age and marital_status\n"
      ],
      "metadata": {
        "id": "Bdlc0eJmrMJT"
      },
      "id": "Bdlc0eJmrMJT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Before we impute missing Marital Status, it is important the dimensions of missing marital status with other columns. So lets filter for Marital status where it is null with the final data frame."
      ],
      "metadata": {
        "id": "UtYqU65XVnmG"
      },
      "id": "UtYqU65XVnmG"
    },
    {
      "cell_type": "code",
      "source": [
        "final_mar = final[final['marital_status'].isnull()]\n",
        "\n",
        "mar_stat = final_mar.groupby(['gender', 'department']).size().unstack(fill_value=0)\n",
        "\n",
        "final_mar.info()"
      ],
      "metadata": {
        "id": "tSslhSJJrQKb"
      },
      "id": "tSslhSJJrQKb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_mar.head()"
      ],
      "metadata": {
        "id": "x2_kQ-8WyRi5"
      },
      "id": "x2_kQ-8WyRi5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fat1 = final.groupby(['department',\n",
        "    'tenure', 'age', 'gender']).size().unstack(fill_value=0)\n",
        "fat1"
      ],
      "metadata": {
        "id": "KRyZHz6wyquC"
      },
      "id": "KRyZHz6wyquC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation** Gender, age is also missing where marital status is missing"
      ],
      "metadata": {
        "id": "16_SIE3XuyUL"
      },
      "id": "16_SIE3XuyUL"
    },
    {
      "cell_type": "code",
      "source": [
        "#Groupby statement to understand modes of the groups of department, tenure and\n",
        "#fat1 = final.groupby(['department',\n",
        "  #  'tenure', 'age', 'gender']).size().unstack(fill_value=0)\n",
        "#fat1"
      ],
      "metadata": {
        "id": "ztdAKpr61Dy7"
      },
      "id": "ztdAKpr61Dy7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# applying mode for filling gender based on department and tenure\n",
        "final['gender'] = final.groupby(['department','tenure'], sort=False)['gender'].apply(lambda x: x.fillna(x.mode().iloc[0]))\n",
        "\n",
        "# applying mode for filling age based on department, tenure and gender\n",
        "final['age'] = final.groupby(['department','tenure', 'gender'], sort=False)['age'].apply(lambda x: x.fillna(x.mode().iloc[0]))\n",
        "# applying mode for filling age based on department, tenure, gender and age\n",
        "final['marital_status'] = final.groupby(['department','tenure', 'gender', 'age'], sort=False)['marital_status'].apply(lambda x: x.fillna(x.mode().iloc[0]))"
      ],
      "metadata": {
        "id": "kM1xwUCQwJ87"
      },
      "id": "kM1xwUCQwJ87",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final.info()"
      ],
      "metadata": {
        "id": "yfrFOQMX1F2S"
      },
      "id": "yfrFOQMX1F2S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final.skew()"
      ],
      "metadata": {
        "id": "HZxcq4D_6gEV"
      },
      "id": "HZxcq4D_6gEV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_eda=final\n",
        "final_eda.info()"
      ],
      "metadata": {
        "id": "9Y4XAX4vUWle"
      },
      "id": "9Y4XAX4vUWle",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Analysis of Department_Temp"
      ],
      "metadata": {
        "id": "ivdL7WmCp3xs"
      },
      "id": "ivdL7WmCp3xs"
    },
    {
      "cell_type": "code",
      "source": [
        "Emp_temp=pre_final[pre_final['department']=='D00-TP']\n",
        "Emp_temp.head()"
      ],
      "metadata": {
        "id": "85E3DMVDjk6-"
      },
      "id": "85E3DMVDjk6-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Emp_temp.describe()"
      ],
      "metadata": {
        "id": "qBHh9RYjmFJb"
      },
      "id": "qBHh9RYjmFJb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use(['seaborn-dark','seaborn-talk'])\n",
        "colors = ( \"orange\", \"cyan\", \"brown\",\n",
        "          \"grey\", \"indigo\", \"beige\")\n",
        "fig, ax = plt.subplots(1,2,figsize=(16,6))\n",
        "\n",
        "Emp_temp['status'].value_counts().plot.pie(explode=[0,0.08], ax=ax[0], autopct='%1.2f%%', shadow=True,\n",
        "                                    fontsize=16, startangle=100, colors=colors)\n",
        "ax[0].set_title('Temp_Dept_Total Employeed vs Lef Percentage')\n",
        "\n",
        "sns.countplot(Emp_temp['status'], data=final, ax=ax[1], palette=['cyan', 'orange'])\n",
        "ax[1].set_title('Temp_Dept_Total Number of temp Employees')\n",
        "ax[1].set_ylabel(' ')\n",
        "for rect in ax[1].patches:\n",
        "    ax[1].text (rect.get_x() + rect.get_width()  / 2,rect.get_height()+ 0.75,rect.get_height(),horizontalalignment='center', fontsize = 20)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UXjhjQaUmb-V"
      },
      "id": "UXjhjQaUmb-V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<a name = Section6></a>\n",
        "# **6. Exploratory Data Analysis**\n",
        "---"
      ],
      "metadata": {
        "id": "c2nnF6-letWD"
      },
      "id": "c2nnF6-letWD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Distribution of Employee Status"
      ],
      "metadata": {
        "id": "6ikJxOGqgFJw"
      },
      "id": "6ikJxOGqgFJw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d296e66",
      "metadata": {
        "scrolled": false,
        "id": "1d296e66"
      },
      "outputs": [],
      "source": [
        "plt.style.use(['seaborn-dark','seaborn-talk'])\n",
        "\n",
        "fig, ax = plt.subplots(1,2,figsize=(16,6))\n",
        "\n",
        "final['status'].value_counts().plot.pie(explode=[0,0.08], ax=ax[0], autopct='%1.2f%%', shadow=False,\n",
        "                                    fontsize=16, startangle=100, colors=['orange', 'cyan'])\n",
        "ax[0].set_title('Percentage distribution of Employee Status')\n",
        "\n",
        "sns.countplot(final['status'], data=final, ax=ax[1], palette=['cyan', 'orange'])\n",
        "ax[1].set_title('Total Number of Employees based on Status')\n",
        "ax[1].set_ylabel(' ')\n",
        "for rect in ax[1].patches:\n",
        "    ax[1].text (rect.get_x() + rect.get_width()  / 2,rect.get_height()+ 0.75,rect.get_height(),horizontalalignment='center', fontsize = 16)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Departmentwise Total Employee percentage"
      ],
      "metadata": {
        "id": "7Wx8GZ24UagE"
      },
      "id": "7Wx8GZ24UagE"
    },
    {
      "cell_type": "code",
      "source": [
        "figure = plt.figure(figsize=[20, 15])\n",
        "\n",
        "\n",
        "# Exploding space based on the number of categories\n",
        "space = np.ones(11)/20\n",
        "\n",
        "# Using magic of pandas pie() function\n",
        "final['dept_name'].value_counts().plot(kind='pie', explode=space, fontsize=14,\n",
        "                                       autopct='%3.1f%%', wedgeprops=dict(width=0.15),\n",
        "                                       shadow=True, startangle=160, cmap='inferno', legend=True)\n",
        "\n",
        "plt.ylabel(ylabel='Category', size=14)\n",
        "plt.title(label='Donut Plot showing department share', size=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uqMKZbXCUZ-q"
      },
      "id": "uqMKZbXCUZ-q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation of Variables"
      ],
      "metadata": {
        "id": "FHcjWp1_gSam"
      },
      "id": "FHcjWp1_gSam"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(30, 20))\n",
        "plt.title('Correlation between variables')\n",
        "sns.heatmap(final.corr(), annot=True, cmap='Blues')"
      ],
      "metadata": {
        "id": "pjVZv0aoUrXp"
      },
      "id": "pjVZv0aoUrXp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attrition by Gender and Salary"
      ],
      "metadata": {
        "id": "_A3V8VI8gWrH"
      },
      "id": "_A3V8VI8gWrH"
    },
    {
      "cell_type": "code",
      "source": [
        "# crosstab\n",
        "pal = ['orange', 'cyan']\n",
        "ax= pd.crosstab(final['gender'], final['status']).apply(lambda r: r/r.sum()*100, axis=1)\n",
        "ax_1 = ax.plot.bar(figsize=(10,10), stacked=False, rot=0, color=pal, fontsize = 16)\n",
        "display(ax)\n",
        "\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.1, 1.0), title=\"status\")\n",
        "\n",
        "plt.xlabel('gender')\n",
        "plt.ylabel('status')\n",
        "#plt.xticks(rotation= -90)\n",
        "\n",
        "for rec in ax_1.patches:\n",
        "    height = rec.get_height()\n",
        "    ax_1.text(rec.get_x() + rec.get_width() / 2,\n",
        "              rec.get_y() + height / 2,\n",
        "              \"{:3.1f}%\".format(height),\n",
        "              ha='center',\n",
        "              va='bottom', fontsize = 16)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D9kAf7v3VVrA"
      },
      "id": "D9kAf7v3VVrA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Department wise Employee Status"
      ],
      "metadata": {
        "id": "kj_63YGbV8GK"
      },
      "id": "kj_63YGbV8GK"
    },
    {
      "cell_type": "code",
      "source": [
        "# crosstab\n",
        "pal = ['orange', 'cyan']\n",
        "ax= pd.crosstab(final['dept_name'], final['status']).apply(lambda r: r/r.sum()*100, axis=1)\n",
        "ax_1 = ax.plot.bar(figsize=(15,7), stacked=False, rot=0, color=pal)\n",
        "display(ax)\n",
        "\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.1, 1.0), title=\"status\")\n",
        "\n",
        "plt.xlabel('dept_name')\n",
        "plt.ylabel('status')\n",
        "plt.xticks(rotation= -90)\n",
        "\n",
        "for rec in ax_1.patches:\n",
        "    height = rec.get_height()\n",
        "    ax_1.text(rec.get_x() + rec.get_width() / 2,\n",
        "              rec.get_y() + height / 2,\n",
        "              \"{:.0f}%\".format(height),\n",
        "              ha='center',\n",
        "              va='bottom', fontsize = 16)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aVgxGihYV7Wz"
      },
      "id": "aVgxGihYV7Wz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Status vs salary"
      ],
      "metadata": {
        "id": "Xvmfc-BYdwuc"
      },
      "id": "Xvmfc-BYdwuc"
    },
    {
      "cell_type": "code",
      "source": [
        "# crosstab\n",
        "pal = ['y', 'r']\n",
        "ax= pd.crosstab(final['salary'], final['status']).apply(lambda r: r/r.sum()*100, axis=1)\n",
        "ax_1 = ax.plot.bar(figsize=(10,6), stacked=False, rot=0, color=pal)\n",
        "display(ax)\n",
        "#bbox_to_anchor=(0.1, 1.0)\n",
        "plt.legend(loc='upper center', title=\"status\")\n",
        "\n",
        "plt.xlabel('salary')\n",
        "plt.ylabel('status')\n",
        "#plt.xticks(rotation= -90)\n",
        "\n",
        "for rec in ax_1.patches:\n",
        "    height = rec.get_height()\n",
        "    ax_1.text(rec.get_x() + rec.get_width() / 2,\n",
        "              rec.get_y() + height / 2,\n",
        "              \"{:.0f}%\".format(height),\n",
        "              ha='center',\n",
        "              va='bottom', fontsize = 14)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jWYuv-5WdyaE"
      },
      "id": "jWYuv-5WdyaE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Subset of Employee_Left for reasons for Attrition"
      ],
      "metadata": {
        "id": "RKmHSHPIUgFy"
      },
      "id": "RKmHSHPIUgFy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8d9434b",
      "metadata": {
        "id": "a8d9434b"
      },
      "outputs": [],
      "source": [
        "Emp_left=final[final['status']=='Left']\n",
        "Emp_left.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33b45d53",
      "metadata": {
        "scrolled": true,
        "id": "33b45d53"
      },
      "outputs": [],
      "source": [
        "Emp_left.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Employee Left - Last evaluation vs Satisfication level by n_projects"
      ],
      "metadata": {
        "id": "7sr1MHbzWmBu"
      },
      "id": "7sr1MHbzWmBu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb7571e3",
      "metadata": {
        "scrolled": false,
        "id": "cb7571e3"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "plt.title(\"Last evaluation vs Satisfication level\")\n",
        "sns.scatterplot(x=Emp_left['satisfaction'],y=Emp_left['last_evaluation'],hue='n_projects',data=Emp_left, palette='bright')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Employee Left - avg_monthly_hrs vs Satisfication level by marital_status"
      ],
      "metadata": {
        "id": "ihQReXRIZArY"
      },
      "id": "ihQReXRIZArY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7e99047",
      "metadata": {
        "scrolled": true,
        "id": "b7e99047"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "plt.title(\"avg_monthly_hrs vs Satisfication level\")\n",
        "sns.scatterplot(x=Emp_left['avg_monthly_hrs'],y=Emp_left['satisfaction'],hue='marital_status',data=Emp_left, palette='bright')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b60386a6",
      "metadata": {
        "id": "b60386a6"
      },
      "source": [
        "Comparison of avg_monthly_hrs vs Satisfication level with marital_status\n",
        "- 1. unmarried people with less average monthly hours are less satisfied and tend to leave the company.\n",
        "- 2. married people with extermely hours average monthly hours are having less satisfaction score are leaving company.\n",
        "- 3. highly satisfied employees with score of 0.8 with average monthly hours more than 200 are leaving company(this inclludes both married and unmarried). This is not quite normal as satisfaction score is high but employees are still leaving."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Employee Left - avg_monthly_hrs vs Satisfication level by salary"
      ],
      "metadata": {
        "id": "kUGIlWIMZNm0"
      },
      "id": "kUGIlWIMZNm0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "818fff91",
      "metadata": {
        "scrolled": true,
        "id": "818fff91"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "plt.title(\"avg_monthly_hrs vs Satisfication level\")\n",
        "sns.scatterplot(x=Emp_left['avg_monthly_hrs'],y=Emp_left['satisfaction'],hue='salary',data=Emp_left, palette='bright')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Employee Left - avg_monthly_hrs vs Satisfication level by gender"
      ],
      "metadata": {
        "id": "eC8LGEvXcxXu"
      },
      "id": "eC8LGEvXcxXu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cda3061",
      "metadata": {
        "scrolled": false,
        "id": "3cda3061"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "plt.title(\"avg_monthly_hrs vs Satisfication level\")\n",
        "sns.scatterplot(x=Emp_left['avg_monthly_hrs'],y=Emp_left['satisfaction'],hue='gender',data=Emp_left, palette='bright')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Employee Left - avg_monthly_hrs vs Satisfication level by n_projects"
      ],
      "metadata": {
        "id": "O5E9ITctcumS"
      },
      "id": "O5E9ITctcumS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88d26c0f",
      "metadata": {
        "scrolled": true,
        "id": "88d26c0f"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "plt.title(\"avg_monthly_hrs vs Satisfication level\")\n",
        "sns.scatterplot(x=Emp_left['avg_monthly_hrs'],y=Emp_left['satisfaction'],hue='n_projects',data=Emp_left, palette='bright')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Box Plot for Salary vs Satisfaction by n_projects"
      ],
      "metadata": {
        "id": "E7PhmtEW29EJ"
      },
      "id": "E7PhmtEW29EJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c754f053",
      "metadata": {
        "scrolled": true,
        "id": "c754f053"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "plt.title('Salary vs Satisfaction level')\n",
        "sns.boxplot(x=Emp_left['salary'],y=Emp_left['satisfaction'],hue='n_projects',data=Emp_left)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Department Analysis of Salary as a contribution to Employee Attrition"
      ],
      "metadata": {
        "id": "nd3bndhQVFPs"
      },
      "id": "nd3bndhQVFPs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a6efacc",
      "metadata": {
        "id": "9a6efacc"
      },
      "outputs": [],
      "source": [
        "dax= pd.crosstab(Emp_left['dept_name'],Emp_left['salary']).apply(lambda r: r/r.sum()*100, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4cc5dac",
      "metadata": {
        "scrolled": true,
        "id": "f4cc5dac"
      },
      "outputs": [],
      "source": [
        "fat = final.groupby([#'filed_complaint',\n",
        "    'status', 'recently_promoted']).size().unstack(fill_value=0)\n",
        "\n",
        "fat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5d89b5b",
      "metadata": {
        "id": "b5d89b5b"
      },
      "outputs": [],
      "source": [
        "fat2 = final.groupby(['filed_complaint', 'status']).size().unstack(fill_value=0)\n",
        "\n",
        "fat2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e94a937c",
      "metadata": {
        "id": "e94a937c"
      },
      "outputs": [],
      "source": [
        "fat3= final.groupby(['recently_promoted', 'dept_name']).size().unstack(fill_value=0)\n",
        "\n",
        "fat3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab1f0d24",
      "metadata": {
        "id": "ab1f0d24"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,7))\n",
        "plt.title('Department vs Satisfaction level by tenure')\n",
        "sns.violinplot(x=Emp_left['dept_name'],y=Emp_left['satisfaction'],hue='tenure',data=Emp_left)\n",
        "#plt.legend(loc='upper right', title=\"tenure\")\n",
        "plt.xticks(rotation= -90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis of Valuable employees"
      ],
      "metadata": {
        "id": "vrluqIqBxuP4"
      },
      "id": "vrluqIqBxuP4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb38992c",
      "metadata": {
        "id": "cb38992c"
      },
      "outputs": [],
      "source": [
        "value_emp=final[(final['satisfaction']>=0.7) & (final['tenure']>=3)]\n",
        "#&final[final['tenure']>3]\n",
        "value_emp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab8eb7a9",
      "metadata": {
        "scrolled": true,
        "id": "ab8eb7a9"
      },
      "outputs": [],
      "source": [
        "value_emp.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35e480e4",
      "metadata": {
        "id": "35e480e4"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "plt.title(\"Value_Emp_Last evaluation vs Satisfication level\")\n",
        "sns.scatterplot(x=value_emp['satisfaction'],y=value_emp['last_evaluation'],hue='status',data=value_emp, palette='bright')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2429c70",
      "metadata": {
        "id": "b2429c70"
      },
      "outputs": [],
      "source": [
        "plt.style.use(['seaborn-dark','seaborn-talk'])\n",
        "\n",
        "fig, ax = plt.subplots(1,2,figsize=(16,6))\n",
        "\n",
        "value_emp['status'].value_counts().plot.pie(explode=[0,0.08], ax=ax[0], autopct='%1.2f%%', shadow=False,\n",
        "                                    fontsize=14, startangle=100, colors=['orange', 'cyan'])\n",
        "ax[0].set_title('Value_Employee Employeed vs Lef Percentage')\n",
        "\n",
        "sns.countplot(value_emp['status'], data=value_emp, ax=ax[1], palette=['cyan', 'orange'])\n",
        "ax[1].set_title('Value_Employee Employeed vs Left count')\n",
        "ax[1].set_ylabel(' ')\n",
        "for rect in ax[1].patches:\n",
        "    ax[1].text (rect.get_x() + rect.get_width()  / 2,rect.get_height()+ 0.75,rect.get_height(),horizontalalignment='center', fontsize = 11)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observation\n",
        "**18% of employees are still leaving from valuable_employees"
      ],
      "metadata": {
        "id": "PN9CSqe3yK7V"
      },
      "id": "PN9CSqe3yK7V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name = Section7></a>\n",
        "\n",
        "---\n",
        "# **7. Data Post-Processing**\n",
        "---"
      ],
      "metadata": {
        "id": "FvE_Igw0Ze8A"
      },
      "id": "FvE_Igw0Ze8A"
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install xverse"
      ],
      "metadata": {
        "id": "KbzouD3z7VwH"
      },
      "id": "KbzouD3z7VwH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final.info()"
      ],
      "metadata": {
        "id": "xyNHA1KT7iV2"
      },
      "id": "xyNHA1KT7iV2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3b0495e",
      "metadata": {
        "id": "f3b0495e"
      },
      "outputs": [],
      "source": [
        "#X = final.drop('status',axis=1)\n",
        "#y = final['status']\n",
        "#from xverse.transformer import WOE\n",
        "#clf = WOE()\n",
        "#clf.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name = Section71></a>\n",
        "### **7.1 Data Encoding**\n",
        "\n",
        "- In this section, we will encode our categorical features such as department, salary, gender, marital_status using one hot encoding."
      ],
      "metadata": {
        "id": "0wO-5ya6aGT5"
      },
      "id": "0wO-5ya6aGT5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing one hot encoding over department, salary, gender, marital_status\n",
        "final_eda = pd.get_dummies(data=final_eda, columns=['department', 'salary', 'gender', 'marital_status'])\n",
        "final_eda.head(2)"
      ],
      "metadata": {
        "id": "7VVqXWqXEFKl"
      },
      "id": "7VVqXWqXEFKl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing Label Encoding of status\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "final_eda['status'] = le.fit_transform(final_eda['status'])"
      ],
      "metadata": {
        "id": "4M8GMs34TvzA"
      },
      "id": "4M8GMs34TvzA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_eda.head()"
      ],
      "metadata": {
        "id": "fv2OY9RQTWYo"
      },
      "id": "fv2OY9RQTWYo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name = Section72></a>\n",
        "### **7.2 Data Scaling**\n",
        "\n",
        "- In this section, we will scale our features to ensure that it one feature doesn't have more impact than others in terms of weights."
      ],
      "metadata": {
        "id": "ZofmfzFZV96Y"
      },
      "id": "ZofmfzFZV96Y"
    },
    {
      "cell_type": "code",
      "source": [
        "# Instatiating input and output data by dropping unnecessary data features\n",
        "X = final_eda.drop(labels=['dept_name', 'dept_head', 'status', 'avg_monthly_hrs','last_evaluation', 'n_projects', 'satisfaction', 'tenure', 'age'], axis=1)\n",
        "y = final_eda['status']\n",
        "\n",
        "# Instatiate a scaler object and performing transformation on age and fare\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(final_eda[['avg_monthly_hrs','last_evaluation', 'n_projects', 'satisfaction', 'tenure', 'age']])\n",
        "data2 = pd.DataFrame(data=scaled_data, columns=['avg_monthly_hrs','last_evaluation', 'n_projects', 'satisfaction', 'tenure', 'age'])\n",
        "data2.head(2)"
      ],
      "metadata": {
        "id": "2mE2hU8rTWQM"
      },
      "id": "2mE2hU8rTWQM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finalX = pd.concat(objs=[X, data2], axis=1)\n",
        "#finalX['int']=1\n",
        "finalX.head()"
      ],
      "metadata": {
        "id": "OHp_JYP7IRW1"
      },
      "id": "OHp_JYP7IRW1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name = Section73></a>\n",
        "### **7.3 Data Preparation**\n",
        "\n",
        "- Now we will **split** our **data** into **dependent** and **independent** variables for further development."
      ],
      "metadata": {
        "id": "e7fu-tT4gdnD"
      },
      "id": "e7fu-tT4gdnD"
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(finalX, y, test_size=0.20, random_state=42, stratify=y)\n",
        "\n",
        "# Display the shape of training and testing data\n",
        "print('X_train shape: ', X_train.shape)\n",
        "print('y_train shape: ', y_train.shape)\n",
        "print('X_test shape: ', X_test.shape)\n",
        "print('y_test shape: ', y_test.shape)"
      ],
      "metadata": {
        "id": "cBEIk9hXgf93"
      },
      "id": "cBEIk9hXgf93",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name = Section8></a>\n",
        "\n",
        "---\n",
        "# **8. Model Development & Evaluation**\n",
        "---\n",
        "\n",
        "- In this section we will **develop a decision Tree model** and Random Forest\n",
        "\n",
        "- Then we will **analyze the results** obtained and **make our observations**.\n",
        "\n",
        "- For **evaluation purpose** we will **focus** on **Precision**, **Recall** and **Accuracy** scores.\n",
        "\n",
        "- We will do **hyperparameter tuning** and see if the scores improves."
      ],
      "metadata": {
        "id": "5UJkDsnwgox6"
      },
      "id": "5UJkDsnwgox6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name = Section81></a>\n",
        "### **8.1 Model Development & Evaluation without tuning**\n",
        "\n",
        "- We will use various models with **KNN Classifier** for **comparison**."
      ],
      "metadata": {
        "id": "SpImCmhnqHLp"
      },
      "id": "SpImCmhnqHLp"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from xgboost.sklearn import XGBClassifier"
      ],
      "metadata": {
        "id": "hMb-GK-Qr-4-"
      },
      "id": "hMb-GK-Qr-4-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating various classifiers\n",
        "clfs = [KNeighborsClassifier(n_neighbors=35),\n",
        "        RandomForestClassifier(n_estimators=30,criterion='gini',random_state=1,max_depth=250, class_weight='balanced'),\n",
        "        GaussianNB(),\n",
        "        GradientBoostingClassifier(n_estimators=150),\n",
        "        #AdaBoostClassifier(n_estimators=80, base_estimator=model,random_state=0),\n",
        "        XGBClassifier(learning_rate =0.000001,n_estimators=1000,max_depth=5,min_child_weight=1,\n",
        "                     subsample=0.8,colsample_bytree=0.8,nthread=4,scale_pos_weight=1,seed=27),\n",
        "        LogisticRegression(class_weight='balanced', max_iter=3000, random_state=0,solver='saga')]\n",
        "\n",
        "for clf in clfs:\n",
        "  # Extracting model name\n",
        "  model_name = type(clf).__name__\n",
        "\n",
        "  # Fit the model on train data\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "  # Make predictions using test data\n",
        "  y_pred_train = clf.predict(X_train)\n",
        "\n",
        "  # Make predictions using test data\n",
        "  y_pred = clf.predict(X_test)\n",
        "\n",
        "  # Calculate the train accuracy of the model\n",
        "  clf_train_accuracy = accuracy_score(y_train, y_pred_train)\n",
        "\n",
        "  # Calculate the test accuracy of the model\n",
        "  clf_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "  # Display the accuracy of the model\n",
        "  print('Performance Metric of', model_name, ':')\n",
        "  print('[Train Accuracy]:', clf_train_accuracy)\n",
        "  print('[Test Accuracy]:', clf_accuracy)\n",
        "  print('----------------------------------------\\n')"
      ],
      "metadata": {
        "id": "tvA24VhLqQyP"
      },
      "id": "tvA24VhLqQyP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name = Section82></a>\n",
        "### **8.2 Baseline Model Development & Evaluation**\n",
        "\n",
        "- Here we will develop random forest classifier using default setting."
      ],
      "metadata": {
        "id": "WTZUZvpggsk4"
      },
      "id": "WTZUZvpggsk4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a Random Forest classifier\n",
        "RF = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
        "RF.fit(X_train, y_train)\n",
        "\n",
        "# Predicting training and testing labels\n",
        "y_train_pred_count = RF.predict(X_train)\n",
        "y_test_pred_count = RF.predict(X_test)\n",
        "\n",
        "# Plotting confusion maxtrix of train and test data\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharex=False, figsize=(15, 7))\n",
        "plot_confusion_matrix(estimator=clf, X=X_train, y_true=y_train, values_format='.5g', cmap='YlGnBu', ax=ax1)\n",
        "plot_confusion_matrix(estimator=clf, X=X_test, y_true=y_test, values_format='.5g', cmap='YlGnBu', ax=ax2)\n",
        "ax1.set_title(label='Train Data', size=14)\n",
        "ax2.set_title(label='Test Data', size=14)\n",
        "ax1.grid(b=False)\n",
        "ax2.grid(b=False)\n",
        "plt.suptitle(t='Confusion Matrix', size=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7w8a9tT0gvps"
      },
      "id": "7w8a9tT0gvps",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_report = classification_report(y_train, y_train_pred_count)\n",
        "test_report = classification_report(y_test, y_test_pred_count)\n",
        "print('                    Training Report          ')\n",
        "print(train_report)\n",
        "print('                    Testing Report           ')\n",
        "print(test_report)"
      ],
      "metadata": {
        "id": "nMEyPNkfhJc2"
      },
      "id": "nMEyPNkfhJc2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**\n",
        "\n",
        "- **Train Data:**\n",
        "  - Model predicted **8610 instances correctly** for **negative class (0=Still Emlpoyed)** while **2685 instances** were predicted **correctly** for **positive class (1=Left)**.\n",
        "  - Model **identified 0 instances positive (1-Left) but in actual** they **were negative class (0=Still Emlpoyed)**.\n",
        "  - Model **identified 1 instance negative (0=Still Emlpoyed)** but in actual it was  **positive (1=Left)**.\n",
        "\n",
        "- **Test Data:**\n",
        "  - Model predicted **2136 instances correctly** for **negative class (0=Still Emlpoyed)** while **620 instances** were predicted **correctly** for **positive class (1=Left)**.\n",
        "  - Model **identified 17 instances positive (1=Left) but in actual** they **were positnegative class (0=Still Emlpoyed)**.\n",
        "  - Model **identified 52 instance negative (0=Still Emlpoyed)** but in actual it was  **positive (1=Left)**.."
      ],
      "metadata": {
        "id": "akSgOKm3cLaF"
      },
      "id": "akSgOKm3cLaF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h3>Feature Importance</h3>**"
      ],
      "metadata": {
        "id": "Obq66756hVWI"
      },
      "id": "Obq66756hVWI"
    },
    {
      "cell_type": "code",
      "source": [
        "features = X_train.columns\n",
        "importances = clf.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "plt.figure(figsize=[15, 7])\n",
        "plt.barh(y=range(len(indices)), width=importances[indices], color='g', align='center')\n",
        "plt.yticks(ticks=range(len(indices)), labels=[features[i] for i in indices], size=12)\n",
        "plt.grid(b=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1TENu27PhVzD"
      },
      "id": "1TENu27PhVzD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h3>Feature selection </h3>**\n",
        "Top features are\n",
        "- satisfaction\n",
        "- tenure\n",
        "- n_projects\n",
        "- avg_monthly_hrs\n",
        "- last_evaluation\n",
        "- age"
      ],
      "metadata": {
        "id": "G6klQ4YDzSMF"
      },
      "id": "G6klQ4YDzSMF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name = Section83></a>\n",
        "### **8.3 Hyperparameter Optimized Model Development & Evaluation**\n",
        "\n",
        "- Here we will develop random forest classifier using some hyperparameter setting.\n",
        "\n",
        "- We will be applying __RandomizedsearchCV__ method for __exhaustive search over specified parameter values__ of estimator.<br/>\n",
        "\n",
        "- To know more about the different parameters in random forest classifier, refer the [**documentation**](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). <br/>\n",
        "\n",
        "- Below we will apply gridsearch over the following parameters:\n",
        "  - criterion\n",
        "  - max_depth\n",
        "  - n_estimators\n",
        "  - min_samples_split\n",
        "  - min_samples_leaf\n",
        "\n",
        "- You can change other parameters also and compare the impact of it via calculating __accuracy score & confusion matrix__"
      ],
      "metadata": {
        "id": "G-AmXdRriHa9"
      },
      "id": "G-AmXdRriHa9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for GridSearchCV\n",
        "# Specify parameters and distributions to sample from\n",
        "param_dist = {\"max_depth\": range(2,7),\n",
        "              \"min_samples_split\": sp_randint(2, 11),\n",
        "              \"min_samples_leaf\": sp_randint(1, 11),\n",
        "              \"bootstrap\": [True, False],\n",
        "              \"n_estimators\": [100, 150, 200, 250],\n",
        "              \"criterion\" : [\"gini\", \"entropy\"],\n",
        "              'max_features': ['sqrt', 'log2', None]\n",
        "             }\n",
        "\n",
        "# Run randomized search\n",
        "random_search = RandomizedSearchCV(estimator=clf,\n",
        "                                   param_distributions=param_dist,\n",
        "                                   n_iter=50,\n",
        "                                   n_jobs=-1)\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Predicting training and testing labels\n",
        "y_train_pred_count = random_search.predict(X_train)\n",
        "y_test_pred_count = random_search.predict(X_test)\n",
        "\n",
        "# Plotting confusion maxtrix of train and test data\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharex=False, figsize=(15, 7))\n",
        "plot_confusion_matrix(estimator=random_search, X=X_train, y_true=y_train, values_format='.5g', cmap='YlGnBu', ax=ax1)\n",
        "plot_confusion_matrix(estimator=random_search, X=X_test, y_true=y_test, values_format='.5g', cmap='YlGnBu', ax=ax2)\n",
        "ax1.set_title(label='Train Data', size=14)\n",
        "ax2.set_title(label='Test Data', size=14)\n",
        "ax1.grid(b=False)\n",
        "ax2.grid(b=False)\n",
        "plt.suptitle(t='Confusion Matrix', size=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cvEWLp3XiI6d"
      },
      "id": "cvEWLp3XiI6d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_report = classification_report(y_train, y_train_pred_count)\n",
        "test_report = classification_report(y_test, y_test_pred_count)\n",
        "print('                    Training Report          ')\n",
        "print(train_report)\n",
        "print('                    Testing Report           ')\n",
        "print(test_report)"
      ],
      "metadata": {
        "id": "cWUQVoSnoA2r"
      },
      "id": "cWUQVoSnoA2r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_search.best_params_"
      ],
      "metadata": {
        "id": "BFJ47SxNd6Bd"
      },
      "id": "BFJ47SxNd6Bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying Best fit model:\n"
      ],
      "metadata": {
        "id": "dBiSNkX9et2n"
      },
      "id": "dBiSNkX9et2n"
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a Random Forest classifier\n",
        "RFB = RandomForestClassifier(n_estimators=100,criterion='entropy',random_state=1,max_depth=6, max_features='sqrt', min_samples_leaf=2, min_samples_split=8, class_weight='balanced')\n",
        "RFB.fit(X_train, y_train)\n",
        "\n",
        "# Predicting training and testing labels\n",
        "y_train_pred_count = RFB.predict(X_train)\n",
        "y_test_pred_count = RFB.predict(X_test)\n",
        "\n",
        "# Plotting confusion maxtrix of train and test data\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharex=False, figsize=(15, 7))\n",
        "plot_confusion_matrix(estimator=RFB, X=X_train, y_true=y_train, values_format='.5g', cmap='YlGnBu', ax=ax1)\n",
        "plot_confusion_matrix(estimator=RFB, X=X_test, y_true=y_test, values_format='.5g', cmap='YlGnBu', ax=ax2)\n",
        "ax1.set_title(label='Train Data', size=14)\n",
        "ax2.set_title(label='Test Data', size=14)\n",
        "ax1.grid(b=False)\n",
        "ax2.grid(b=False)\n",
        "plt.suptitle(t='Confusion Matrix', size=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cJCM8uIwe9s0"
      },
      "id": "cJCM8uIwe9s0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_report = classification_report(y_train, y_train_pred_count)\n",
        "test_report = classification_report(y_test, y_test_pred_count)\n",
        "print('                    Training Report          ')\n",
        "print(train_report)\n",
        "print('                    Testing Report           ')\n",
        "print(test_report)"
      ],
      "metadata": {
        "id": "UOYYPgPBfRsZ"
      },
      "id": "UOYYPgPBfRsZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name = Section9></a>\n",
        "\n",
        "---\n",
        "# **9. Unseen Data**\n",
        "---\n",
        "\n",
        "- In this section we will **import the unseen data** and apply hyper tuned model of Random Forest\n",
        "\n",
        "- Then we will **analyze the results** obtained and **make our observations**.\n"
      ],
      "metadata": {
        "id": "U2Eoz54ij1uO"
      },
      "id": "U2Eoz54ij1uO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unseen Data"
      ],
      "metadata": {
        "id": "WwZiAX_k97R5"
      },
      "id": "WwZiAX_k97R5"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZStAUTDm4xRg"
      },
      "id": "ZStAUTDm4xRg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unseen_data = pd.read_excel('/content/drive/MyDrive/CAPSTONE2/unseen_data.xlsx')\n",
        "print('Shape of the unseen dataset:', unseen_data.shape)\n",
        "unseen_data.head()"
      ],
      "metadata": {
        "id": "3KnzZXFy9T-t"
      },
      "id": "3KnzZXFy9T-t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tasks to be done:\n",
        "handle missing values\n",
        "standard scaling\n",
        "predict"
      ],
      "metadata": {
        "id": "7_2G95k8-D5v"
      },
      "id": "7_2G95k8-D5v"
    },
    {
      "cell_type": "code",
      "source": [
        "unseen_data.info()"
      ],
      "metadata": {
        "id": "qWFWqho6-AV4"
      },
      "id": "qWFWqho6-AV4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unseen_data=unseen_data.replace(['-IT'],['D00-IT'])"
      ],
      "metadata": {
        "id": "MjfH_Eg3JNRc"
      },
      "id": "MjfH_Eg3JNRc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge1_uns=pd.merge(unseen_data, df_emp1, on=\"employee_id\", how='left')\n",
        "print('Shape of the merge1 dataset:', merge1_uns.shape)"
      ],
      "metadata": {
        "id": "xpbmidlb_0M0"
      },
      "id": "xpbmidlb_0M0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge1_uns.info()"
      ],
      "metadata": {
        "id": "O5uZbNRFAhta"
      },
      "id": "O5uZbNRFAhta",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling the missing values of department feature with the mode of the feature.\n",
        "merge1_uns['department'] = merge1_uns['department'].fillna(value=final['department'].mode()[0])"
      ],
      "metadata": {
        "id": "is7w96UYA49q"
      },
      "id": "is7w96UYA49q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values = {\"filed_complaint\" : 0, \"recently_promoted\" : 0}\n",
        "merge1_uns.fillna(value = values, inplace = True)\n",
        "merge1_uns.info()"
      ],
      "metadata": {
        "id": "6vrju-O7BP7e"
      },
      "id": "6vrju-O7BP7e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge1_uns['last_evaluation'].fillna(value=merge1_uns['last_evaluation'].mean(), inplace=True)\n",
        "merge1_uns['satisfaction'].fillna(value=merge1_uns['satisfaction'].mean(), inplace=True)\n",
        "merge1_uns['tenure'].fillna(value=merge1_uns['tenure'].mean(), inplace=True)"
      ],
      "metadata": {
        "id": "hxwDoJrW-j1R"
      },
      "id": "hxwDoJrW-j1R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(merge1_uns[['avg_monthly_hrs','last_evaluation', 'n_projects', 'satisfaction', 'tenure', 'age']])\n",
        "data3 = pd.DataFrame(data=scaled_data, columns=['avg_monthly_hrs','last_evaluation', 'n_projects', 'satisfaction', 'tenure', 'age'])\n",
        "data3.head(2)"
      ],
      "metadata": {
        "id": "WqSdavg2LfHn"
      },
      "id": "WqSdavg2LfHn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Dummies"
      ],
      "metadata": {
        "id": "oH7Toxx1CQW3"
      },
      "id": "oH7Toxx1CQW3"
    },
    {
      "cell_type": "code",
      "source": [
        "merge1_uns = pd.get_dummies(data=merge1_uns, columns=['department', 'salary', 'gender', 'marital_status'])\n",
        "merge1_uns.head(2)\n"
      ],
      "metadata": {
        "id": "TW7CczbMCSnG"
      },
      "id": "TW7CczbMCSnG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X2 = merge1_uns.drop(labels=['avg_monthly_hrs','last_evaluation', 'n_projects', 'satisfaction', 'tenure', 'age'], axis=1)"
      ],
      "metadata": {
        "id": "R9ACcLbEMEAN"
      },
      "id": "R9ACcLbEMEAN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uns_finalX = pd.concat(objs=[X2, data3], axis=1)\n",
        "#finalX['int']=1\n",
        "uns_finalX.head()"
      ],
      "metadata": {
        "id": "ekQQhSmkLzP-"
      },
      "id": "ekQQhSmkLzP-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge1_uns.info()"
      ],
      "metadata": {
        "id": "eyggFjvXIvoe"
      },
      "id": "eyggFjvXIvoe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicting on the RF model evaluated"
      ],
      "metadata": {
        "id": "JQb9KoLRK3uu"
      },
      "id": "JQb9KoLRK3uu"
    },
    {
      "cell_type": "code",
      "source": [
        "random_search.best_params_"
      ],
      "metadata": {
        "id": "sAozClKCeXrD"
      },
      "id": "sAozClKCeXrD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "# Instantiate a decision tree classifier\n",
        "RF = RandomForestClassifier(n_estimators=100,criterion='entropy',random_state=1,max_depth=6, max_features='sqrt',\n",
        "                            min_samples_leaf=2, min_samples_split=8, class_weight='balanced')\n",
        "RF.fit(X_train, y_train)\n",
        "\n",
        "# Predicting training and testing labels\n",
        "#y_train_pred_count = clf.predict(X_train)\n",
        "y_test_pred_count = RF.predict(uns_finalX)\n",
        "y_test_pred_count"
      ],
      "metadata": {
        "id": "Yu5ZhpdQBcm9"
      },
      "id": "Yu5ZhpdQBcm9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predproba_uns_finalX=RF.predict_proba(uns_finalX)"
      ],
      "metadata": {
        "id": "g6KBHT-6DGsp"
      },
      "id": "g6KBHT-6DGsp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba_uns_finalX2 = y_predproba_uns_finalX[:,1]\n",
        "y_proba_uns_finalX2"
      ],
      "metadata": {
        "id": "1O3xRqC_EaXd"
      },
      "id": "1O3xRqC_EaXd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "employee_id = unseen_data.employee_id\n",
        "employee_id_List = employee_id.tolist()"
      ],
      "metadata": {
        "id": "59Bsoi_OGuaC"
      },
      "id": "59Bsoi_OGuaC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Final_Pred_unseen_data2 = pd.DataFrame(list(zip(employee_id_List, y_test_pred_count,y_proba_uns_finalX2)),columns =['employee_id_List', 'Prediction','PredictProba'])"
      ],
      "metadata": {
        "id": "RZOjBhA2GB1T"
      },
      "id": "RZOjBhA2GB1T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## exporting the results to csv"
      ],
      "metadata": {
        "id": "4tdhCX8EK8z8"
      },
      "id": "4tdhCX8EK8z8"
    },
    {
      "cell_type": "code",
      "source": [
        "res = pd.DataFrame(y_proba_uns_finalX2)\n",
        "res.index = merge1_uns.index\n",
        "#res.index = merge1_uns['avg_monthly_hrs',\t'department',\t'filed_complaint',\t'last_evaluation',\t'n_projects',\t'recently_promoted',\t'salary',\t'satisfaction',\t'tenure', 'employee_id']\n",
        "#res.index = merge1_uns ***ValueError: Index data must be 1-dimensional***\n",
        "res.index = merge1_uns['employee_id']\n",
        "res.columns = [\"status\"]\n",
        "res.to_csv(\"attrition_prediction_results\", index = True, header = False)"
      ],
      "metadata": {
        "id": "3RIU18iiJZ4F"
      },
      "id": "3RIU18iiJZ4F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###File with Employee ID Probability Score"
      ],
      "metadata": {
        "id": "3Tv2LwMahufh"
      },
      "id": "3Tv2LwMahufh"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "res.to_csv('attrition_prediction_results.csv')\n",
        "files.download('attrition_prediction_results.csv')"
      ],
      "metadata": {
        "id": "YG6WnHanKLPy"
      },
      "id": "YG6WnHanKLPy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###File with Employee ID, Status and Probability Score"
      ],
      "metadata": {
        "id": "SdjlmOTxhnp_"
      },
      "id": "SdjlmOTxhnp_"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "Final_Pred_unseen_data2.to_csv('prob_results.csv')\n",
        "files.download('prob_results.csv')"
      ],
      "metadata": {
        "id": "8Vr7M9-XHUWU"
      },
      "id": "8Vr7M9-XHUWU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### how to read the results:\n",
        "Results to be read as 0= likely to stay in the company\n",
        "1= likely to leave the company"
      ],
      "metadata": {
        "id": "L8EBN0xJLGdq"
      },
      "id": "L8EBN0xJLGdq"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2484c8a1",
        "997ff614"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}